{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sad folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to the folder containing images\n",
    "folder_path = r\"\"\n",
    "\n",
    "# Path to the new folder for sad images\n",
    "output_folder_sad = os.path.join(folder_path, \"sad1\")\n",
    "\n",
    "# Path to the new folder for non-sad images (EXTRAS)\n",
    "output_folder_extras = r\"\"\n",
    "\n",
    "# Create the output folders if they don't exist\n",
    "if not os.path.exists(output_folder_sad):\n",
    "    os.makedirs(output_folder_sad)\n",
    "\n",
    "if not os.path.exists(output_folder_extras):\n",
    "    os.makedirs(output_folder_extras)\n",
    "\n",
    "# Dictionary to store emotion counts\n",
    "emotion_count = defaultdict(int)\n",
    "\n",
    "# Iterate through all images in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Check if the file is an image (you can add more extensions if needed)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        try:\n",
    "            # Analyze the image for emotions\n",
    "            result = DeepFace.analyze(img_path=file_path, actions=['emotion'], enforce_detection=False)\n",
    "            \n",
    "            # Get the dominant emotion\n",
    "            dominant_emotion = result[0]['dominant_emotion']\n",
    "            \n",
    "            # Update the emotion count\n",
    "            emotion_count[dominant_emotion] += 1\n",
    "            \n",
    "            # If the dominant emotion is \"sad\", copy the image to the sad folder\n",
    "            if dominant_emotion == \"sad\":\n",
    "                output_path = os.path.join(output_folder_sad, filename)\n",
    "                shutil.copy(file_path, output_path)\n",
    "                print(f\"File: {filename} | Dominant Emotion: {dominant_emotion} | Copied to {output_folder_sad}\")\n",
    "            \n",
    "            # If the dominant emotion is not \"sad\", move the image to the EXTRAS folder\n",
    "            else:\n",
    "                output_path = os.path.join(output_folder_extras, filename)\n",
    "                shutil.copy(file_path, output_path)\n",
    "                print(f\"File: {filename} | Dominant Emotion: {dominant_emotion} | Copied to {output_folder_extras}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Print the final emotion counts\n",
    "print(\"\\nEmotion counts in the folder:\")\n",
    "for emotion, count in emotion_count.items():\n",
    "    print(f\"{emotion}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import wandb\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    " \n",
    " \n",
    "# Define paths\n",
    "shape_predictor_path = r\"\"\n",
    "happy_folder = r''\n",
    "sad_folder = r''\n",
    "aligned_happy = r''\n",
    "aligned_sad = r''\n",
    " \n",
    "# Create preprocessed directories if they don't exist\n",
    "os.makedirs(aligned_happy, exist_ok=True)\n",
    "os.makedirs(aligned_sad, exist_ok=True)\n",
    " \n",
    "# Initialize dlib's face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shape_predictor_path)\n",
    " \n",
    "# Block 2: Face Alignment Functions\n",
    " \n",
    "def align_and_save_faces(source_folder, target_folder):\n",
    "    \"\"\"Preprocess and save aligned faces to target folder\"\"\"\n",
    "    if len(os.listdir(target_folder)) > 0:\n",
    "        return  # Skip if already processed\n",
    " \n",
    "    for img_name in os.listdir(source_folder):\n",
    "        img_path = os.path.join(source_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        aligned = align_face(image)\n",
    "        if aligned:\n",
    "            aligned.save(os.path.join(target_folder, img_name))\n",
    " \n",
    "def align_face(image):\n",
    "    \"\"\"Align face using facial landmarks\"\"\"\n",
    "    try:\n",
    "        image_np = np.array(image)\n",
    "        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "        faces = detector(gray, 1)\n",
    "        if not faces:\n",
    "            return None\n",
    "        landmarks = predictor(gray, faces[0])\n",
    "        landmarks = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
    "        desired_left_eye = (0.35, 0.35)\n",
    "        desired_face_width = 256\n",
    "        desired_face_height = 256\n",
    "        left_eye_center = landmarks[36:42].mean(axis=0)\n",
    "        right_eye_center = landmarks[42:48].mean(axis=0)\n",
    "        dY = right_eye_center[1] - left_eye_center[1]\n",
    "        dX = right_eye_center[0] - left_eye_center[0]\n",
    "        angle = np.degrees(np.arctan2(dY, dX))\n",
    "        dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
    "        desired_dist = (1.0 - 2 * desired_left_eye[0]) * desired_face_width\n",
    "        scale = desired_dist / dist\n",
    "        eyes_center = ((left_eye_center[0] + right_eye_center[0]) // 2,\n",
    "                       (left_eye_center[1] + right_eye_center[1]) // 2)\n",
    "        M = cv2.getRotationMatrix2D(eyes_center, angle, scale)\n",
    "        tX = desired_face_width * 0.5\n",
    "        tY = desired_face_height * desired_left_eye[1]\n",
    "        M[0, 2] += (tX - eyes_center[0])\n",
    "        M[1, 2] += (tY - eyes_center[1])\n",
    "        aligned_face = cv2.warpAffine(image_np, M, (desired_face_width, desired_face_height), flags=cv2.INTER_CUBIC)\n",
    "        return Image.fromarray(aligned_face)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    " \n",
    "# Preprocess datasets once before training\n",
    "print(\"Preprocessing datasets...\")\n",
    "#align_and_save_faces(happy_folder, aligned_happy)\n",
    "align_and_save_faces(sad_folder, aligned_sad)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "happy_checking_folder = r''\n",
    "sad_checking_folder = r''\n",
    "\n",
    "# Count the number of images in each folder\n",
    "happy_checking_images = [f for f in os.listdir(happy_checking_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "sad_checking_images = [f for f in os.listdir(sad_checking_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156014"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(happy_checking_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41032"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sad_checking_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COPYING IMAGES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying happyinitial: 100%|██████████| 155599/155599 [02:45<00:00, 942.23it/s] \n",
      "Copying sadinitial: 100%|██████████| 41032/41032 [00:35<00:00, 1152.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ADJUSTING FOLDERS TO 10,000 IMAGES ===\n",
      "\n",
      "Final counts:\n",
      "Happy: 10000 images\n",
      "Sad: 10000 images\n",
      "Extras moved to: C:\\Users\\Asus\\Desktop\\ATML\\Project\\LetsTry\\preprocessed\\extras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "source_happy = r''\n",
    "source_sad = r''\n",
    "verified_happy = r''\n",
    "verified_sad = r''\n",
    "extras_folder = r''\n",
    "target_count = 10000  # New target count for both folders\n",
    "\n",
    "\n",
    "def copy_all_images(source_folder, target_folder):\n",
    "    \"\"\"Copy all images from source to target folder\"\"\"\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    images = [f for f in os.listdir(source_folder) \n",
    "             if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    for img in tqdm(images, desc=f\"Copying {os.path.basename(source_folder)}\"):\n",
    "        src = os.path.join(source_folder, img)\n",
    "        dst = os.path.join(target_folder, img)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "def adjust_folder(folder, target_count, extras_folder):\n",
    "    \"\"\"Adjust folder to have exactly target_count images\"\"\"\n",
    "    images = os.listdir(folder)\n",
    "    current_count = len(images)\n",
    "    \n",
    "    if current_count > target_count:\n",
    "        # Downsample - move extras\n",
    "        keep_images = random.sample(images, target_count)\n",
    "        for img in images:\n",
    "            if img not in keep_images:\n",
    "                shutil.move(os.path.join(folder, img), \n",
    "                           os.path.join(extras_folder, img))\n",
    "    elif current_count < target_count:\n",
    "        # Upsample - create duplicates\n",
    "        needed = target_count - current_count\n",
    "        for i in tqdm(range(needed), desc=f\"Upsampling {os.path.basename(folder)}\"):\n",
    "            img = random.choice(images)\n",
    "            base, ext = os.path.splitext(img)\n",
    "            new_name = f\"{base}_dup{i}{ext}\"\n",
    "            shutil.copy(os.path.join(folder, img),\n",
    "                       os.path.join(folder, new_name))\n",
    "\n",
    "# ===== MAIN PROCESS =====\n",
    "try:\n",
    "    print(\"=== COPYING IMAGES ===\")\n",
    "    copy_all_images(source_happy, verified_happy)\n",
    "    copy_all_images(source_sad, verified_sad)\n",
    "    \n",
    "    print(\"\\n=== ADJUSTING FOLDERS TO 10,000 IMAGES ===\")\n",
    "    os.makedirs(extras_folder, exist_ok=True)\n",
    "    \n",
    "    # Adjust both folders to have exactly target_count images\n",
    "    adjust_folder(verified_happy, target_count, extras_folder)\n",
    "    adjust_folder(verified_sad, target_count, extras_folder)\n",
    "    \n",
    "    # Final counts\n",
    "    happy_count = len(os.listdir(verified_happy))\n",
    "    sad_count = len(os.listdir(verified_sad))\n",
    "    \n",
    "    print(f\"\\nFinal counts:\")\n",
    "    print(f\"Happy: {happy_count} images\")\n",
    "    print(f\"Sad: {sad_count} images\")\n",
    "    print(f\"Extras moved to: {extras_folder}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n!!! ERROR: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: c9ox8yvd\n",
      "Sweep URL: https://wandb.ai/utkarshrawat04-nmims/sad-happy-translation/sweeps/c9ox8yvd\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization\n",
    "    'metric': {\n",
    "        'name': 'G_loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'lr_g': {\n",
    "            'min': 1e-5,\n",
    "            'max': 2e-4,\n",
    "            'distribution': 'uniform'\n",
    "        },\n",
    "        'lr_d': {\n",
    "            'min': 1e-5,\n",
    "            'max': 2e-4,\n",
    "            'distribution': 'uniform'\n",
    "        },\n",
    "        'lambda_cycle': {\n",
    "            'min': 5.0,\n",
    "            'max': 15.0,\n",
    "            'distribution': 'uniform'\n",
    "        },\n",
    "        'lambda_identity': {\n",
    "            'min': 0.5,\n",
    "            'max': 5.0,\n",
    "            'distribution': 'uniform'\n",
    "        },\n",
    "        'num_residual_blocks': {\n",
    "            'values': [6, 9, 12]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"sad-happy-translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (keep all your original imports here)\n",
    "\n",
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'G_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr_G': {\n",
    "            'min': 1e-6,\n",
    "            'max': 2e-4\n",
    "        },\n",
    "        'lr_D': {\n",
    "            'min': 1e-6,\n",
    "            'max': 2e-4\n",
    "        },\n",
    "        'beta1': {\n",
    "            'min': 0.4,\n",
    "            'max': 0.9\n",
    "        },\n",
    "        'beta2': {\n",
    "            'min': 0.95,\n",
    "            'max': 0.9999\n",
    "        },\n",
    "        'lambda_cycle': {\n",
    "            'min': 3.0,\n",
    "            'max': 10.0\n",
    "        },\n",
    "        'lambda_identity': {\n",
    "            'min': 0.5,\n",
    "            'max': 5.0\n",
    "        },\n",
    "        'lambda_perceptual': {\n",
    "            'min': 0.1,\n",
    "            'max': 2.0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"face-expression-translation\")\n",
    "\n",
    "# Keep your dataset and dataloader setup unchanged\n",
    "verified_happy = r''\n",
    "verified_sad = r''\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize wandb run\n",
    "    wandb.init()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize models\n",
    "    G_happyTosad = Generator().to(device)\n",
    "    G_sadTohappy = Generator().to(device)\n",
    "    D_happy = Discriminator().to(device)\n",
    "    D_sad = Discriminator().to(device)\n",
    "\n",
    "    # Get hyperparameters from wandb.config\n",
    "    config = wandb.config\n",
    "    lambda_cycle = config.lambda_cycle\n",
    "    lambda_identity = config.lambda_identity\n",
    "    lambda_perceptual = config.lambda_perceptual\n",
    "\n",
    "    # Initialize optimizers with sweep parameters\n",
    "    G_optimizer = optim.Adam(\n",
    "        list(G_happyTosad.parameters()) + list(G_sadTohappy.parameters()),\n",
    "        lr=config.lr_G,\n",
    "        betas=(config.beta1, config.beta2)\n",
    "    )\n",
    "    D_optimizer = optim.Adam(\n",
    "        list(D_happy.parameters()) + list(D_sad.parameters()),\n",
    "        lr=config.lr_D,\n",
    "        betas=(config.beta1, config.beta2)\n",
    "    )\n",
    "\n",
    "    # Keep loss functions and VGG setup unchanged\n",
    "    criterion_gan = nn.MSELoss()\n",
    "    criterion_cycle = nn.L1Loss()\n",
    "    criterion_identity = nn.L1Loss()\n",
    "    vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training loop with hyperparameters\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # ... (keep your existing training loop code here)\n",
    "        \n",
    "        # Modify the loss calculation to use sweep parameters\n",
    "        with autocast():\n",
    "            # ... (existing code)\n",
    "            \n",
    "            # Total generator loss\n",
    "            G_loss = (\n",
    "                loss_gan_happyTosad + loss_gan_sadTohappy +\n",
    "                lambda_cycle * (loss_cycle_happy + loss_cycle_sad) +\n",
    "                lambda_identity * (loss_identity_sad + loss_identity_happy) +\n",
    "                lambda_perceptual * (loss_perceptual_sad + loss_perceptual_happy)\n",
    "            )\n",
    "\n",
    "        # Log additional hyperparameters\n",
    "        wandb.log({\n",
    "            **{k: v for k, v in config.items()},\n",
    "            \"G_loss\": G_loss.item(),\n",
    "            \"D_loss\": D_loss.item(),\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "\n",
    "\n",
    "    torch.save(G_happyTosad.state_dict(), f'G_happyTosad_{wandb.run.id}.pth')\n",
    "    torch.save(G_sadTohappy.state_dict(), f'G_sadTohappy_{wandb.run.id}.pth')\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train, count=50)  # Adjust count as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import wandb\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "wandb.init(project=\"face-expression-translation\")\n",
    "\n",
    "verified_happy = r''\n",
    "verified_sad = r''\n",
    "\n",
    "# Block 3: Dataset and DataLoader\n",
    " \n",
    "class PreprocessedDataset(Dataset):\n",
    "    \"\"\"Dataset that loads preprocessed images\"\"\"\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.image_paths = [os.path.join(folder, f) for f in os.listdir(folder)]\n",
    "        self.transform = transform\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = np.array(image)  # Convert PIL image to numpy array\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image\n",
    "\n",
    "# Define transformations with Albumentations\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    " \n",
    "# Create datasets and dataloaders\n",
    "print(\"Creating dataloaders...\")\n",
    "happy_dataset = PreprocessedDataset(verified_happy, transform)\n",
    "sad_dataset = PreprocessedDataset(verified_sad, transform)\n",
    " \n",
    "happy_dataloader = DataLoader(\n",
    "    happy_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    " \n",
    "sad_dataloader = DataLoader(\n",
    "    sad_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    " \n",
    "# Block 4: Generator and Discriminator Models (Modified for PatchGAN)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.down1 = self.conv_block(3, 64, normalize=False)\n",
    "        self.down2 = self.conv_block(64, 128)\n",
    "        self.down3 = self.conv_block(128, 256)\n",
    "        self.down4 = self.conv_block(256, 512)\n",
    "        self.down5 = self.conv_block(512, 512)\n",
    "        self.up1 = self.deconv_block(512, 512)\n",
    "        self.up2 = self.deconv_block(1024, 256)\n",
    "        self.up3 = self.deconv_block(512, 128)\n",
    "        self.up4 = self.deconv_block(256, 64)\n",
    "        self.up5 = self.deconv_block(128, 3, normalize=False, activation=nn.Tanh())\n",
    "        self.apply(self._init_weights)\n",
    " \n",
    "    def conv_block(self, in_channels, out_channels, normalize=True, activation=nn.LeakyReLU(0.2)):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(activation)\n",
    "        return nn.Sequential(*layers)\n",
    " \n",
    "    def deconv_block(self, in_channels, out_channels, normalize=True, activation=nn.ReLU()):\n",
    "        layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(activation)\n",
    "        return nn.Sequential(*layers)\n",
    " \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0.0)\n",
    " \n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        u1 = self.up1(d5)\n",
    "        u2 = self.up2(torch.cat([u1, d4], dim=1))\n",
    "        u3 = self.up3(torch.cat([u2, d3], dim=1))\n",
    "        u4 = self.up4(torch.cat([u3, d2], dim=1))\n",
    "        u5 = self.up5(torch.cat([u4, d1], dim=1))\n",
    "        return u5\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"PatchGAN discriminator\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)  # Outputs 30x30 patch predictions\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Output shape: (batch_size, 1, 30, 30)\n",
    "\n",
    "# Block 5: Training Setup (No changes needed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "G_happyTosad = Generator().to(device)\n",
    "G_sadTohappy = Generator().to(device)\n",
    "D_happy = Discriminator().to(device)\n",
    "D_sad = Discriminator().to(device)\n",
    " \n",
    "G_optimizer = optim.Adam(list(G_happyTosad.parameters()) + list(G_sadTohappy.parameters()), lr=0.00005, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(list(D_happy.parameters()) + list(D_sad.parameters()), lr=0.00005, betas=(0.5, 0.999))\n",
    " \n",
    "criterion_gan = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    " \n",
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    " \n",
    "def perceptual_loss(generated, real, vgg_model):\n",
    "    def extract_features(x, vgg_model):\n",
    "        features = []\n",
    "        for layer_num, layer in enumerate(vgg_model):\n",
    "            x = layer(x)\n",
    "            if layer_num in [2, 7, 12, 21, 30]:\n",
    "                features.append(x)\n",
    "        return features\n",
    "    generated_features = extract_features(generated, vgg_model)\n",
    "    real_features = extract_features(real, vgg_model)\n",
    "    loss = 0\n",
    "    for gen_feature, real_feature in zip(generated_features, real_features):\n",
    "        loss += F.l1_loss(gen_feature, real_feature)\n",
    "    return loss\n",
    " \n",
    "scaler = GradScaler()\n",
    "num_epochs = 100\n",
    "lambda_cycle = 5.0\n",
    "lambda_identity = 2.0\n",
    "lambda_perceptual = 0.5\n",
    "\n",
    "# Block 6: Training Loop (No changes needed)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nStarting Epoch {epoch+1}/{num_epochs}\")\n",
    " \n",
    "    with tqdm(total=len(happy_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for i, (happy_images, sad_images) in enumerate(zip(happy_dataloader, sad_dataloader)):\n",
    "            iter_start = time.time()\n",
    "            happy_images = happy_images.to(device, non_blocking=True)\n",
    "            sad_images = sad_images.to(device, non_blocking=True)\n",
    " \n",
    "            # Generator training\n",
    "            G_happyTosad.zero_grad()\n",
    "            G_sadTohappy.zero_grad()\n",
    "    \n",
    "            with autocast():\n",
    "                # Identity losses\n",
    "                identity_sad = G_happyTosad(sad_images)\n",
    "                loss_identity_sad = criterion_identity(identity_sad, sad_images)\n",
    "                identity_happy = G_sadTohappy(happy_images)\n",
    "                loss_identity_happy = criterion_identity(identity_happy, happy_images)\n",
    "    \n",
    "                # GAN losses\n",
    "                fake_sad = G_happyTosad(happy_images)\n",
    "                fake_sad_pred = D_sad(fake_sad)\n",
    "                loss_gan_happyTosad = criterion_gan(fake_sad_pred, torch.ones_like(fake_sad_pred))\n",
    "    \n",
    "                fake_happy = G_sadTohappy(sad_images)\n",
    "                fake_happy_pred = D_happy(fake_happy)\n",
    "                loss_gan_sadTohappy = criterion_gan(fake_happy_pred, torch.ones_like(fake_happy_pred))\n",
    "    \n",
    "                # Cycle losses\n",
    "                reconstructed_happy = G_sadTohappy(fake_sad)\n",
    "                loss_cycle_happy = criterion_cycle(reconstructed_happy, happy_images)\n",
    "                reconstructed_sad = G_happyTosad(fake_happy)\n",
    "                loss_cycle_sad = criterion_cycle(reconstructed_sad, sad_images)\n",
    "    \n",
    "                # Perceptual losses\n",
    "                loss_perceptual_sad = perceptual_loss(fake_sad, sad_images, vgg)\n",
    "                loss_perceptual_happy = perceptual_loss(fake_happy, happy_images, vgg)\n",
    "    \n",
    "                # Total generator loss\n",
    "                G_loss = (\n",
    "                    loss_gan_happyTosad + loss_gan_sadTohappy +\n",
    "                    lambda_cycle * (loss_cycle_happy + loss_cycle_sad) +\n",
    "                    lambda_identity * (loss_identity_sad + loss_identity_happy) +\n",
    "                    lambda_perceptual * (loss_perceptual_sad + loss_perceptual_happy)\n",
    "                )\n",
    "    \n",
    "            scaler.scale(G_loss).backward()\n",
    "            scaler.step(G_optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            # Discriminator training\n",
    "            D_happy.zero_grad()\n",
    "            D_sad.zero_grad()\n",
    "    \n",
    "            with autocast():\n",
    "                # Real images\n",
    "                real_happy_pred = D_happy(happy_images)\n",
    "                loss_real_happy = criterion_gan(real_happy_pred, torch.ones_like(real_happy_pred))\n",
    "                real_sad_pred = D_sad(sad_images)\n",
    "                loss_real_sad = criterion_gan(real_sad_pred, torch.ones_like(real_sad_pred))\n",
    "    \n",
    "                # Fake images\n",
    "                fake_happy_pred = D_happy(fake_happy.detach())\n",
    "                loss_fake_happy = criterion_gan(fake_happy_pred, torch.zeros_like(fake_happy_pred))\n",
    "                fake_sad_pred = D_sad(fake_sad.detach())\n",
    "                loss_fake_sad = criterion_gan(fake_sad_pred, torch.zeros_like(fake_sad_pred))\n",
    "    \n",
    "                D_loss = (loss_real_happy + loss_fake_happy + loss_real_sad + loss_fake_sad) / 2\n",
    "    \n",
    "            scaler.scale(D_loss).backward()\n",
    "            scaler.step(D_optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            # Logging\n",
    "            if i % 20 == 0:\n",
    "                iter_time = time.time() - iter_start\n",
    "                pbar.set_postfix({\n",
    "                    \"Time\": f\"{iter_time:.2f}s\",\n",
    "                    \"G_loss\": f\"{G_loss.item():.4f}\",\n",
    "                    \"D_loss\": f\"{D_loss.item():.4f}\"\n",
    "                })\n",
    "            pbar.update(1)\n",
    " \n",
    "        # Epoch logging\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1} completed in {epoch_time//60:.0f}m {epoch_time%60:.0f}s\")\n",
    "        wandb.log({\n",
    "            \"G_loss\": G_loss.item(),\n",
    "            \"D_loss\": D_loss.item(),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": i\n",
    "        })\n",
    "        \n",
    "        # Save sample images\n",
    "        with torch.no_grad():\n",
    "            fake_sad = G_happyTosad(happy_images)\n",
    "            fake_happy = G_sadTohappy(sad_images)\n",
    "            save_image(fake_sad, f'fake_sad_epoch_{epoch+1}.png', normalize=True)\n",
    "            save_image(fake_happy, f'fake_happy_epoch_{epoch+1}.png', normalize=True)\n",
    "            wandb.log({\n",
    "                \"Fake sad\": [wandb.Image(fake_sad[0], caption=f\"Epoch {epoch+1}\")],\n",
    "                \"Fake happy\": [wandb.Image(fake_happy[0], caption=f\"Epoch {epoch+1}\")]\n",
    "            })\n",
    " \n",
    "# Block 7: Model Saving and Wandb Finish\n",
    "torch.save(G_happyTosad.state_dict(), 'G_happyTosad.pth')\n",
    "torch.save(G_sadTohappy.state_dict(), 'G_sadTohappy.pth')\n",
    "torch.save(D_happy.state_dict(), 'D_happy.pth')\n",
    "torch.save(D_sad.state_dict(), 'D_sad.pth')\n",
    " \n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import dlib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Generator model (matches PatchGAN-trained architecture)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.down1 = self.conv_block(3, 64, normalize=False)\n",
    "        self.down2 = self.conv_block(64, 128)\n",
    "        self.down3 = self.conv_block(128, 256)\n",
    "        self.down4 = self.conv_block(256, 512)\n",
    "        self.down5 = self.conv_block(512, 512)\n",
    "        self.up1 = self.deconv_block(512, 512)\n",
    "        self.up2 = self.deconv_block(1024, 256)\n",
    "        self.up3 = self.deconv_block(512, 128)\n",
    "        self.up4 = self.deconv_block(256, 64)\n",
    "        self.up5 = self.deconv_block(128, 3, normalize=False, activation=nn.Tanh())\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, normalize=True, activation=nn.LeakyReLU(0.2)):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(activation)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def deconv_block(self, in_channels, out_channels, normalize=True, activation=nn.ReLU()):\n",
    "        layers = [nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(activation)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        u1 = self.up1(d5)\n",
    "        u2 = self.up2(torch.cat([u1, d4], 1))\n",
    "        u3 = self.up3(torch.cat([u2, d3], 1))\n",
    "        u4 = self.up4(torch.cat([u3, d2], 1))\n",
    "        u5 = self.up5(torch.cat([u4, d1], 1))\n",
    "        return u5\n",
    "\n",
    "# Load trained models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "G_happyTosad = Generator().to(device)\n",
    "G_sadTohappy = Generator().to(device)\n",
    "G_happyTosad.load_state_dict(torch.load('G_happyTosad.pth', map_location=device))\n",
    "G_sadTohappy.load_state_dict(torch.load('G_sadTohappy.pth', map_location=device))\n",
    "G_happyTosad.eval()\n",
    "G_sadTohappy.eval()\n",
    "\n",
    "# Define transformations (matches training preprocessing)\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Initialize dlib components\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def align_face(image):\n",
    "    \"\"\"Robust face alignment implementation with error handling\"\"\"\n",
    "    try:\n",
    "        image_np = np.array(image)\n",
    "        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Detect faces with upsampling for better detection\n",
    "        faces = detector(gray, 1)\n",
    "        if not faces:\n",
    "            print(\"No faces detected\")\n",
    "            return None\n",
    "            \n",
    "        # Use the first detected face\n",
    "        face = faces[0]\n",
    "        \n",
    "        # Get facial landmarks with error checking\n",
    "        landmarks = predictor(gray, face)\n",
    "        if not landmarks:\n",
    "            print(\"No landmarks detected\")\n",
    "            return None\n",
    "            \n",
    "        # Convert landmarks to numpy array with integer coordinates\n",
    "        landmarks = np.array([[p.x, p.y] for p in landmarks.parts()], dtype=np.int32)\n",
    "        if landmarks.size == 0:\n",
    "            print(\"Empty landmarks array\")\n",
    "            return None\n",
    "            \n",
    "        # Calculate eye positions with bounds checking\n",
    "        try:\n",
    "            # Left eye landmarks (points 36-41)\n",
    "            left_eye_points = landmarks[36:42]\n",
    "            # Right eye landmarks (points 42-47)\n",
    "            right_eye_points = landmarks[42:48]\n",
    "            \n",
    "            if len(left_eye_points) < 6 or len(right_eye_points) < 6:\n",
    "                print(\"Incomplete eye landmarks\")\n",
    "                return None\n",
    "                \n",
    "            left_eye_center = left_eye_points.mean(axis=0).astype(np.int32)\n",
    "            right_eye_center = right_eye_points.mean(axis=0).astype(np.int32)\n",
    "            \n",
    "        except IndexError as e:\n",
    "            print(f\"Landmark indexing error: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        # Calculate angle between eyes\n",
    "        dY = right_eye_center[1] - left_eye_center[1]\n",
    "        dX = right_eye_center[0] - left_eye_center[0]\n",
    "        angle = np.degrees(np.arctan2(dY, dX))\n",
    "        \n",
    "        # Calculate desired scale\n",
    "        desired_left_eye = (0.35, 0.35)\n",
    "        desired_face_width = 256\n",
    "        desired_face_height = 256\n",
    "        \n",
    "        # Determine scale of new image\n",
    "        dist = np.sqrt((dX**2) + (dY**2))\n",
    "        desired_dist = (1.0 - 2 * desired_left_eye[0]) * desired_face_width\n",
    "        scale = desired_dist / max(dist, 1e-6)  # Prevent division by zero\n",
    "        \n",
    "        # Find center point between eyes\n",
    "        eyes_center = (\n",
    "            int((left_eye_center[0] + right_eye_center[0]) // 2),\n",
    "            int((left_eye_center[1] + right_eye_center[1]) // 2)\n",
    "        )\n",
    "        \n",
    "        # Calculate rotation matrix\n",
    "        M = cv2.getRotationMatrix2D(eyes_center, angle, scale)\n",
    "        \n",
    "        # Update translation components\n",
    "        tX = desired_face_width * 0.5\n",
    "        tY = desired_face_height * desired_left_eye[1]\n",
    "        M[0, 2] += (tX - eyes_center[0])\n",
    "        M[1, 2] += (tY - eyes_center[1])\n",
    "        \n",
    "        # Apply affine transformation\n",
    "        (w, h) = (desired_face_width, desired_face_height)\n",
    "        aligned_face = cv2.warpAffine(\n",
    "            image_np, M, (w, h),\n",
    "            flags=cv2.INTER_CUBIC,\n",
    "            borderMode=cv2.BORDER_REPLICATE\n",
    "        )\n",
    "        \n",
    "        return Image.fromarray(aligned_face)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Alignment error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    aligned_image = align_face(image)\n",
    "    if aligned_image is None:\n",
    "        raise ValueError(\"No face detected in the image\")\n",
    "    \n",
    "    image_np = np.array(aligned_image)\n",
    "    transformed = transform(image=image_np)\n",
    "    return transformed['image'].unsqueeze(0).to(device)\n",
    "\n",
    "def generate_and_display_images(input_image_path, generator, title):\n",
    "    \"\"\"Complete generation and display workflow\"\"\"\n",
    "    try:\n",
    "        # Load and align\n",
    "        input_image = Image.open(input_image_path).convert('RGB')\n",
    "        aligned_image = align_face(input_image)\n",
    "        if aligned_image is None:\n",
    "            print(\"Skipping image - no face detected\")\n",
    "            return\n",
    "\n",
    "        # Preprocess\n",
    "        input_tensor = transform(image=np.array(aligned_image))['image']\n",
    "        input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = generator(input_tensor)\n",
    "\n",
    "        # Postprocess\n",
    "        generated_np = generated.squeeze().cpu().numpy()\n",
    "        generated_np = np.transpose(generated_np, (1, 2, 0))\n",
    "        generated_np = (generated_np * 0.5 + 0.5) * 255\n",
    "        generated_np = generated_np.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        # Visualize\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(aligned_image)\n",
    "        plt.title(\"Aligned Input\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(generated_np)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Generation failed: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "test_cases = [\n",
    "    (r\"\", G_happyTosad, \"Generated Sad Expression\")\n",
    "]\n",
    "\n",
    "for img_path, model, title in test_cases:\n",
    "    generate_and_display_images(img_path, model, title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
